# ExerciseLLM
Python code and dataset repository for paper "insert_paper_name"
KITE Research Institute 2024

[[Project Page]](insert_link) [[Paper]](insert_link) 


(add image of architecture/visualizations)

(add citations block)


## Table of Content
* [1. Results](#1-results)
* [2. Installation](#2-installation)
* [3. Datasets](#3-datasets)
* [4. Evaluation](#4-evaluation)
* [5. Acknowledgements](#5-acknowledgements)


## 1. Results 
(insert results images, text, etc)
 
## 2. Installation

### 2.1. Environment
CONDA ENVIRONMENT.YML OR PIP REQUIREMENTS.TXT? 

```bash
conda env create -f environment.yml
conda activate exerLLM
```


## 3. Datasets
(list links and download and sample file directory tree)

UI-PRMD: 
- using only segmented files from Kinect

```
./dataset/
├── UI-PRMD
    ├── correct
        ├── kinect
            ├── angles/
            ├── positions
                ├── m01_s01_e01_positions.txt
                ├── m01_s01_e02_positions.txt
                ├── ...
                ├── m10_s10_e10_positions.txt
    ├── incorrect/
└── UI-PRMD_visualization.ipynb
```

### 3.1. Preparation files
Position to absolute coordinates: 
Feature extractors:

## 4. Evaluation 
Automatic prompt generation:



## 5. Visualization
Visualize UI-PRMD movements by running `visualization/UI-PRMD_visualization.ipynb`

### 6. Acknowledgements

* public code 
* (add contributor names)
