# ExerciseLLM
Pytorch implementation of paper "insert_paper_name"
KITE Research Institute 2024

[[Project Page]](insert_link) [[Paper]](insert_link) 


(add image of architecture/visualizations)

(add citations block)


## Table of Content
* [1. Visual Results](#1-visual-results)
* [2. Installation](#2-installation)
* [3. Quick Start](#3-quick-start)
* [4. Train](#4-train)
* [5. Evaluation](#5-evaluation)
* [6. SMPL Mesh Rendering](#6-smpl-mesh-rendering)
* [7. Acknowledgement](#7-acknowledgement)
* [8. ChangLog](#8-changlog)



## 1. Visual Results 
(insert results images, text, etc)
 
## 2. Installation

### 2.1. Environment
CONDA ENVIRONMENT.YML OR PIP REQUIREMENTS.TXT? 

```bash
conda env create -f environment.yml
conda activate exerLLM
```

### 2.2. Dependencies
??

### 2.3. Datasets
(list links and download and sample file directory tree)

### 2.4. Motion & text feature extractors:
??

### 2.5. Pre-trained models 
??

## 3. Train
??

### 3.1. VQ-VAE 



## 4. Evaluation 

## 5. Visualization


### 6. Acknowledgements

* public code 
* (add contributor names)