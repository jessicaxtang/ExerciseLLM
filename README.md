# ExerciseLLM
Pytorch implementation of paper "insert_paper_name"
KITE Research Institute 2024

[[Project Page]](insert_link) [[Paper]](insert_link) 


(add image of architecture/visualizations)

(add citations block)


## Table of Content
* [1. Results](#1-results)
* [2. Installation](#2-installation)
* [3. Train](#4-train)
* [4. Evaluation](#5-evaluation)
* [5. Visualization](#6-visualization)
* [6. Acknowledgements](#7-acknowledgements)



## 1. Results 
(insert results images, text, etc)
 
## 2. Installation

### 2.1. Environment
CONDA ENVIRONMENT.YML OR PIP REQUIREMENTS.TXT? 

```bash
conda env create -f environment.yml
conda activate exerLLM
```

### 2.2. Dependencies
??

### 2.3. Datasets
(list links and download and sample file directory tree)

UI-PRMD: 
- using only segmented files from Kinect

```
./dataset/UI-PRMD/
├── UI-PRMD
    ├── Correct
        ├── Kinect
            ├── Angles/
            ├── Positions
                |-- m01_s01_e01_positions.txt
                |-- m01_s01_e02_positions.txt
                |-- ...
                |-- m10_s10_e10_positions.txt
    |-- Incorrect/
└── UI-PRMD_visualization.ipynb
```

### 2.4. Motion & text feature extractors:
??

### 2.5. Pre-trained models 
??

## 3. Train
??

### 3.1. VQ-VAE 



## 4. Evaluation 

## 5. Visualization
Visualize UI-PRMD movements by running `UI-PRMD/UI-PRMD_visualization.ipynb`

### 6. Acknowledgements

* public code 
* (add contributor names)